{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Homework 1: Part A2: Linear Regression with multiple variables\n",
    "## Experiments with the Boston housing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "This file contains code that helps you get started on linear regression with many variables. You will need to complete functions in **linear_regressor_multi.py** and **utils.py**. The only changes to make in this notebook are marked with **TODO**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "will start by loading and displaying some values from the full Boston housing dataset with thirteen features of census tracts that are believed to be predictive of the median home price in the tract (see **housing.names.txt** for a full description of these features). By looking at the values, you will note that the values of some of the features are  about 1000 times the values of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.25179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.570</td>\n",
       "      <td>98.1</td>\n",
       "      <td>3.7979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.57</td>\n",
       "      <td>21.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.965</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>392.53</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.98843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.54</td>\n",
       "      <td>19.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.75026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.924</td>\n",
       "      <td>94.1</td>\n",
       "      <td>4.3996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.33</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.84054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.599</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4.4546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>303.42</td>\n",
       "      <td>16.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.67191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>90.3</td>\n",
       "      <td>4.6820</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.88</td>\n",
       "      <td>14.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.95577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.047</td>\n",
       "      <td>88.8</td>\n",
       "      <td>4.4534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>306.38</td>\n",
       "      <td>17.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.77299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.495</td>\n",
       "      <td>94.4</td>\n",
       "      <td>4.4547</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>387.94</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.674</td>\n",
       "      <td>87.3</td>\n",
       "      <td>4.2390</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.23</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4.87141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.484</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2.3053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.21</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>349.48</td>\n",
       "      <td>24.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>10.23300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.185</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.1705</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>379.70</td>\n",
       "      <td>18.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.229</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>383.32</td>\n",
       "      <td>13.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>5.82401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.242</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.4242</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>5.70818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.750</td>\n",
       "      <td>74.9</td>\n",
       "      <td>3.3317</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>393.07</td>\n",
       "      <td>7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>5.73116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>7.061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.28</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.81838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>5.762</td>\n",
       "      <td>40.3</td>\n",
       "      <td>4.0983</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.92</td>\n",
       "      <td>10.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2.37857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.871</td>\n",
       "      <td>41.9</td>\n",
       "      <td>3.7240</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>370.73</td>\n",
       "      <td>13.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.68</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4.83567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.905</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.1523</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.22</td>\n",
       "      <td>11.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.15086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.454</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>395.09</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>344.05</td>\n",
       "      <td>23.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.707</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.17899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.670</td>\n",
       "      <td>28.8</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>393.29</td>\n",
       "      <td>17.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.28960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.390</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>21.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.26838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.794</td>\n",
       "      <td>70.6</td>\n",
       "      <td>2.8927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.23912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.019</td>\n",
       "      <td>65.3</td>\n",
       "      <td>2.4091</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>395.77</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "0     0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n",
       "1     0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671   2.0  242.0   \n",
       "2     0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671   2.0  242.0   \n",
       "3     0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622   3.0  222.0   \n",
       "4     0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622   3.0  222.0   \n",
       "5     0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605   5.0  311.0   \n",
       "7     0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505   5.0  311.0   \n",
       "8     0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821   5.0  311.0   \n",
       "9     0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921   5.0  311.0   \n",
       "10    0.22489  12.5   7.87   0.0  0.524  6.377   94.3  6.3467   5.0  311.0   \n",
       "11    0.11747  12.5   7.87   0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
       "12    0.09378  12.5   7.87   0.0  0.524  5.889   39.0  5.4509   5.0  311.0   \n",
       "13    0.62976   0.0   8.14   0.0  0.538  5.949   61.8  4.7075   4.0  307.0   \n",
       "14    0.63796   0.0   8.14   0.0  0.538  6.096   84.5  4.4619   4.0  307.0   \n",
       "15    0.62739   0.0   8.14   0.0  0.538  5.834   56.5  4.4986   4.0  307.0   \n",
       "16    1.05393   0.0   8.14   0.0  0.538  5.935   29.3  4.4986   4.0  307.0   \n",
       "17    0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579   4.0  307.0   \n",
       "18    0.80271   0.0   8.14   0.0  0.538  5.456   36.6  3.7965   4.0  307.0   \n",
       "19    0.72580   0.0   8.14   0.0  0.538  5.727   69.5  3.7965   4.0  307.0   \n",
       "20    1.25179   0.0   8.14   0.0  0.538  5.570   98.1  3.7979   4.0  307.0   \n",
       "21    0.85204   0.0   8.14   0.0  0.538  5.965   89.2  4.0123   4.0  307.0   \n",
       "22    1.23247   0.0   8.14   0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "23    0.98843   0.0   8.14   0.0  0.538  5.813  100.0  4.0952   4.0  307.0   \n",
       "24    0.75026   0.0   8.14   0.0  0.538  5.924   94.1  4.3996   4.0  307.0   \n",
       "25    0.84054   0.0   8.14   0.0  0.538  5.599   85.7  4.4546   4.0  307.0   \n",
       "26    0.67191   0.0   8.14   0.0  0.538  5.813   90.3  4.6820   4.0  307.0   \n",
       "27    0.95577   0.0   8.14   0.0  0.538  6.047   88.8  4.4534   4.0  307.0   \n",
       "28    0.77299   0.0   8.14   0.0  0.538  6.495   94.4  4.4547   4.0  307.0   \n",
       "29    1.00245   0.0   8.14   0.0  0.538  6.674   87.3  4.2390   4.0  307.0   \n",
       "..        ...   ...    ...   ...    ...    ...    ...     ...   ...    ...   \n",
       "476   4.87141   0.0  18.10   0.0  0.614  6.484   93.6  2.3053  24.0  666.0   \n",
       "477  15.02340   0.0  18.10   0.0  0.614  5.304   97.3  2.1007  24.0  666.0   \n",
       "478  10.23300   0.0  18.10   0.0  0.614  6.185   96.7  2.1705  24.0  666.0   \n",
       "479  14.33370   0.0  18.10   0.0  0.614  6.229   88.0  1.9512  24.0  666.0   \n",
       "480   5.82401   0.0  18.10   0.0  0.532  6.242   64.7  3.4242  24.0  666.0   \n",
       "481   5.70818   0.0  18.10   0.0  0.532  6.750   74.9  3.3317  24.0  666.0   \n",
       "482   5.73116   0.0  18.10   0.0  0.532  7.061   77.0  3.4106  24.0  666.0   \n",
       "483   2.81838   0.0  18.10   0.0  0.532  5.762   40.3  4.0983  24.0  666.0   \n",
       "484   2.37857   0.0  18.10   0.0  0.583  5.871   41.9  3.7240  24.0  666.0   \n",
       "485   3.67367   0.0  18.10   0.0  0.583  6.312   51.9  3.9917  24.0  666.0   \n",
       "486   5.69175   0.0  18.10   0.0  0.583  6.114   79.8  3.5459  24.0  666.0   \n",
       "487   4.83567   0.0  18.10   0.0  0.583  5.905   53.2  3.1523  24.0  666.0   \n",
       "488   0.15086   0.0  27.74   0.0  0.609  5.454   92.7  1.8209   4.0  711.0   \n",
       "489   0.18337   0.0  27.74   0.0  0.609  5.414   98.3  1.7554   4.0  711.0   \n",
       "490   0.20746   0.0  27.74   0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "491   0.10574   0.0  27.74   0.0  0.609  5.983   98.8  1.8681   4.0  711.0   \n",
       "492   0.11132   0.0  27.74   0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "493   0.17331   0.0   9.69   0.0  0.585  5.707   54.0  2.3817   6.0  391.0   \n",
       "494   0.27957   0.0   9.69   0.0  0.585  5.926   42.6  2.3817   6.0  391.0   \n",
       "495   0.17899   0.0   9.69   0.0  0.585  5.670   28.8  2.7986   6.0  391.0   \n",
       "496   0.28960   0.0   9.69   0.0  0.585  5.390   72.9  2.7986   6.0  391.0   \n",
       "497   0.26838   0.0   9.69   0.0  0.585  5.794   70.6  2.8927   6.0  391.0   \n",
       "498   0.23912   0.0   9.69   0.0  0.585  6.019   65.3  2.4091   6.0  391.0   \n",
       "499   0.17783   0.0   9.69   0.0  0.585  5.569   73.5  2.3999   6.0  391.0   \n",
       "500   0.22438   0.0   9.69   0.0  0.585  6.027   79.7  2.4982   6.0  391.0   \n",
       "501   0.06263   0.0  11.93   0.0  0.573  6.593   69.1  2.4786   1.0  273.0   \n",
       "502   0.04527   0.0  11.93   0.0  0.573  6.120   76.7  2.2875   1.0  273.0   \n",
       "503   0.06076   0.0  11.93   0.0  0.573  6.976   91.0  2.1675   1.0  273.0   \n",
       "504   0.10959   0.0  11.93   0.0  0.573  6.794   89.3  2.3889   1.0  273.0   \n",
       "505   0.04741   0.0  11.93   0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "5       18.7  394.12   5.21  \n",
       "6       15.2  395.60  12.43  \n",
       "7       15.2  396.90  19.15  \n",
       "8       15.2  386.63  29.93  \n",
       "9       15.2  386.71  17.10  \n",
       "10      15.2  392.52  20.45  \n",
       "11      15.2  396.90  13.27  \n",
       "12      15.2  390.50  15.71  \n",
       "13      21.0  396.90   8.26  \n",
       "14      21.0  380.02  10.26  \n",
       "15      21.0  395.62   8.47  \n",
       "16      21.0  386.85   6.58  \n",
       "17      21.0  386.75  14.67  \n",
       "18      21.0  288.99  11.69  \n",
       "19      21.0  390.95  11.28  \n",
       "20      21.0  376.57  21.02  \n",
       "21      21.0  392.53  13.83  \n",
       "22      21.0  396.90  18.72  \n",
       "23      21.0  394.54  19.88  \n",
       "24      21.0  394.33  16.30  \n",
       "25      21.0  303.42  16.51  \n",
       "26      21.0  376.88  14.81  \n",
       "27      21.0  306.38  17.28  \n",
       "28      21.0  387.94  12.80  \n",
       "29      21.0  380.23  11.98  \n",
       "..       ...     ...    ...  \n",
       "476     20.2  396.21  18.68  \n",
       "477     20.2  349.48  24.91  \n",
       "478     20.2  379.70  18.03  \n",
       "479     20.2  383.32  13.11  \n",
       "480     20.2  396.90  10.74  \n",
       "481     20.2  393.07   7.74  \n",
       "482     20.2  395.28   7.01  \n",
       "483     20.2  392.92  10.42  \n",
       "484     20.2  370.73  13.34  \n",
       "485     20.2  388.62  10.58  \n",
       "486     20.2  392.68  14.98  \n",
       "487     20.2  388.22  11.45  \n",
       "488     20.1  395.09  18.06  \n",
       "489     20.1  344.05  23.97  \n",
       "490     20.1  318.43  29.68  \n",
       "491     20.1  390.11  18.07  \n",
       "492     20.1  396.90  13.35  \n",
       "493     19.2  396.90  12.01  \n",
       "494     19.2  396.90  13.59  \n",
       "495     19.2  393.29  17.60  \n",
       "496     19.2  396.90  21.14  \n",
       "497     19.2  396.90  14.10  \n",
       "498     19.2  396.90  12.92  \n",
       "499     19.2  395.77  15.10  \n",
       "500     19.2  396.90  14.33  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plot_utils, utils\n",
    "from linear_regressor_multi import LinearRegressor_Multi, LinearReg_SquaredLoss\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print 'Reading data ...'\n",
    "bdata = load_boston()\n",
    "df = pd.DataFrame(data = bdata.data, columns = bdata.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with multiple variables\n",
    "When features differ by orders of magnitude, feature scaling becomes important\n",
    "to make  gradient descent converge  quickly.\n",
    "**Your task here is to complete the code in feature_normalize.py in utils.py**. \n",
    "- First, subtract the mean value of each feature from the dataset. \n",
    "- Second, divide the feature values by their respective standard deviations. The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within two standard deviations of the mean).  \n",
    "\n",
    "You will do this for all the features and your code should work with\n",
    "datasets of all sizes (any number of features/examples). Note that each\n",
    "column of the matrix X corresponds to one feature.\n",
    "When normalizing the features, it is important\n",
    "to store the values used for normalization - the mean value and the standard deviation used for the computations.\n",
    "\n",
    "Then, run the computation in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values\n",
    "y = bdata.target\n",
    "\n",
    "# need to scale the features (use zero mean scaling)\n",
    "\n",
    "X_norm,mu,sigma = utils.feature_normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and gradient descent (vectorized)\n",
    "Previously, you implemented gradient descent on a univariate regression problem. The only difference now is that there are more features in the matrix X. The hypothesis function and the batch gradient descent update\n",
    "rule remain unchanged. You should complete the code for the train method and the loss method at the indicated points\n",
    "in **linear_regressor_multi.py** to implement the cost function and gradient descent for linear regression with\n",
    "multiple variables.  Make sure your code supports any number of features and that it is **vectorized**.\n",
    "I recommend the use of  numpy's code vectorization facilities. You should see the cost $J(\\theta)$ converge as shown in Figure 5 of the assignment handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gradient descent ..\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106622bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta computed by gradient descent:  [  2.25328063e+01  -9.13925619e-01   1.06949712e+00   1.07531669e-01\n",
      "   6.87258582e-01  -2.05340341e+00   2.67719690e+00   1.55788957e-02\n",
      "  -3.10668099e+00   2.56946272e+00  -1.97453430e+00  -2.05873147e+00\n",
      "   8.55982884e-01  -3.74517559e+00]\n"
     ]
    }
   ],
   "source": [
    "# add intercept term to X_norm\n",
    "\n",
    "XX = np.vstack([np.ones((X.shape[0],)),X_norm.T]).T\n",
    "\n",
    "print 'Running gradient descent ..'\n",
    "\n",
    "# set up model and train \n",
    "\n",
    "linear_reg3 = LinearReg_SquaredLoss()\n",
    "J_history3 = linear_reg3.train(XX,y,learning_rate=0.01,num_iters=5000,verbose=False)\n",
    "\n",
    "# Plot the convergence graph and show it (or save it in fig5.pdf)\n",
    "\n",
    "plot_utils.plot_data(range(len(J_history3)),J_history3,'Number of iterations','Cost J')\n",
    "plt.show()\n",
    "\n",
    "# Display the computed theta\n",
    "\n",
    "print 'Theta computed by gradient descent: ', linear_reg3.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on unseen data points\n",
    "After learning the parameter $\\theta$, we  want to predict the median home prices for new census tracts. \n",
    "Given the thirteen characteristics x of a new census tract, we must first normalize x using the mean and standard deviations that we had previously computed from the training set. Then, we take the dot product of the normalized x (with a 1 prepended (to account for the intercept term) with the parameter vector $\\theta$ to make a prediction.\n",
    "In the cell below, your  final parameter values for $\\theta$ will  be used to make predictions on median home values\n",
    "for an average census tract, characterized by average values for all the thirteen features.  Complete the\n",
    "calculation in  the indicated lines below. Now run this cell to see what the  prediction of median home value for an average tract is. Remember to scale the features correctly for this prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For average home in Boston suburbs, we predict a median home value of 225328.063241\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "# Predict values for the average home                                  #\n",
    "# remember to multiply prediction by 10000 using linear_reg3           #\n",
    "#   One line of code expected; replace pred_cost = 0 line              # \n",
    "########################################################################\n",
    "\n",
    "pred_cost = linear_reg3.predict(np.mean(XX, axis = 0)) * 10000\n",
    "print 'For average home in Boston suburbs, we predict a median home value of', pred_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equations\n",
    "Using the closed form solution for $\\theta$ does not require any feature scaling, and you will get\n",
    "an exact solution in one calculation: there is no loop until convergence\n",
    "as in gradient descent.\n",
    "Complete the code in the method *normal_eqn* in *linear_regressor_multi.py* to  calculate $\\theta$. Now make a prediction for the average census tract (same example as in the previous problem). Do the predictions match up? Remember that while you do not need to scale your features, you still\n",
    "need to add a 1 to the example to have an intercept term ($\\theta_0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta computed by direct solution is:  [  3.64911033e+01  -1.07170557e-01   4.63952195e-02   2.08602395e-02\n",
      "   2.68856140e+00  -1.77957587e+01   3.80475246e+00   7.51061703e-04\n",
      "  -1.47575880e+00   3.05655038e-01  -1.23293463e-02  -9.53463555e-01\n",
      "   9.39251272e-03  -5.25466633e-01]\n",
      "For average home in Boston suburbs, we predict a median home value of 225328.063241\n"
     ]
    }
   ],
   "source": [
    "X = df.values\n",
    "y = bdata.target\n",
    "XX1 = np.vstack([np.ones((X.shape[0],)),X.T]).T\n",
    "\n",
    "linear_reg4 = LinearReg_SquaredLoss()\n",
    "\n",
    "theta_n = linear_reg4.normal_equation(XX1,y)\n",
    "\n",
    "print 'Theta computed by direct solution is: ', theta_n\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "# Predict values for the average home using theta_n                    #\n",
    "# remember to multiply prediction by 10000                             #\n",
    "#   One line of code expected; replace pred_cost = 0 line              # \n",
    "########################################################################\n",
    "\n",
    "pred_cost = np.dot(np.mean(XX1, axis = 0), theta_n) * 10000\n",
    "print 'For average home in Boston suburbs, we predict a median home value of', pred_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring convergence of gradient descent\n",
    "In this part of the exercise, you will get to try out different learning rates for\n",
    "the dataset and find a learning rate that converges quickly. You can change\n",
    "the learning rate and the number of iterations by modifying the call to the **LinearReg** constructor in the cell below.\n",
    "The next phase will call your train function and run gradient descent  at the chosen learning\n",
    "rate for the chosen number of iterations. The function should also return the history of $J(\\theta)$ values in a vector\n",
    "$J$. After the last iteration, the  script plots the $J$ values against\n",
    "the number of the iterations.\n",
    "If you picked a learning rate within a good range, your plot should look similar\n",
    "to Figure 5. If your graph looks very different, especially if your value of $J(\\theta)$\n",
    "increases or even blows up, adjust your learning rate and try again. We recommend trying values of the learning rate $\\alpha$ on a log-scale, at multiplicative\n",
    "steps of about 3 times the previous value (i.e., 0.3, 0.1, 0.03, 0.01 and so on).\n",
    "You may also want to adjust the number of iterations you are running if that\n",
    "will help you see the overall trend in the curve. Present plots of $J$ as a function of the number of iterations for different learning rates. What are good learning rates and number of iterations for this problem? Include plots and a brief writeup in **writeup.pdf** to justify your choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n",
      "10.9488896088\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a16722150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change the learning_rate and num_iters in the call below to find the \n",
    "# best learning rate for this data set.\n",
    "\n",
    "learning_rates = [0.01, 0.03, 0.1, 0.3]\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "# Produce convergence plots for gradient descent with the rates above  #\n",
    "# using data (XX,y). Include them in your writeup.                     #\n",
    "#   4-5 lines of code expected                                         #\n",
    "########################################################################\n",
    "num_iters = [500, 1000, 5000, 10000]\n",
    "best_lr = 0\n",
    "\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for num_iter in num_iters:\n",
    "        J_history3 = linear_reg3.train(XX, y, learning_rate = lr, num_iters = num_iter, verbose = False)\n",
    "        plt.plot(range(len(J_history)), J_history, color[lr] +'o')\n",
    "        ##plt.subplot(2, 2, i+1)\n",
    "        ##plt.plot(range(len(J_history3)),J_history3)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "plt.legend(['learning rates = %s' %lr])\n",
    "        print (J_history3[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
